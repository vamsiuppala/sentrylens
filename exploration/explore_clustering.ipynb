{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring HDBSCANClusterer\n",
    "Interactive notebook to understand clusterer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from src.sentrylens.clustering.clusterer import HDBSCANClusterer\n",
    "from src.sentrylens.core.models import ErrorEmbedding\n",
    "from configs.settings import settings\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"Embedding dimension: {settings.EMBEDDING_DIMENSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 2: Load Actual Embeddings from Data"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector = np.random.randn(settings.EMBEDDING_DIMENSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load actual embeddings from data file\nfrom src.sentrylens.data.loader import AERIDataLoader\nfrom pathlib import Path\n\n# Use the most recent embeddings file\nembeddings_file = Path(\"data/embeddings/embeddings_20260120_132919.json\")\n\nif embeddings_file.exists():\n    loader = AERIDataLoader()\n    dataset = loader.load_processed_dataset(embeddings_file)\n    test_embeddings = dataset.embeddings\n    test_errors = dataset.errors\n    \n    print(f\"Loaded {len(test_embeddings)} embeddings from {embeddings_file.name}\")\n    print(f\"Also loaded {len(test_errors)} errors\")\n    print(f\"\\nFirst embedding:\")\n    print(f\"  error_id: {test_embeddings[0].error_id}\")\n    print(f\"  embedding dimension: {len(test_embeddings[0].embedding)}\")\n    print(f\"  model: {test_embeddings[0].model_name}\")\nelse:\n    print(f\"File not found: {embeddings_file}\")\n    print(\"Run Step 2 first: python -m scripts.generate_embeddings --input <processed_dataset.json>\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Explore - Initialize HDBSCANClusterer\n",
    "What does the __init__ method do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"text\": \"2026-01-20 21:19:36.128 | INFO     | src.sentrylens.clustering.clusterer:__init__:93 - Initialized HDBSCANClusterer\\n\", \"record\": {\"elapsed\": {\"repr\": \"0:58:29.982367\", \"seconds\": 3509.982367}, \"exception\": null, \"extra\": {\"min_cluster_size\": 3, \"min_samples\": 3, \"cluster_selection_epsilon\": 0.0, \"algorithm\": \"best\", \"metric\": \"euclidean\"}, \"file\": {\"name\": \"clusterer.py\", \"path\": \"/Users/vamsi.uppala/Documents/personal/sentrylens/src/sentrylens/clustering/clusterer.py\"}, \"function\": \"__init__\", \"level\": {\"icon\": \"ℹ️\", \"name\": \"INFO\", \"no\": 20}, \"line\": 93, \"message\": \"Initialized HDBSCANClusterer\", \"module\": \"clusterer\", \"name\": \"src.sentrylens.clustering.clusterer\", \"process\": {\"id\": 67926, \"name\": \"MainProcess\"}, \"thread\": {\"id\": 8387567744, \"name\": \"MainThread\"}, \"time\": {\"repr\": \"2026-01-20 21:19:36.128624-08:00\", \"timestamp\": 1768972776.128624}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'algorithm', 'cluster_embeddings', 'cluster_selection_epsilon', 'clusterer', 'embeddings', 'fit', 'get_cluster_center', 'get_cluster_members', 'get_stats', 'labels', 'metric', 'min_cluster_size', 'min_samples', 'predict']\n"
     ]
    }
   ],
   "source": [
    "# Try creating a clusterer and inspect it\n",
    "clusterer = HDBSCANClusterer(min_cluster_size=3)\n",
    "\n",
    "# TODO: Explore the clusterer object\n",
    "# - What attributes does it have?\n",
    "# - Print min_cluster_size, min_samples, metric\n",
    "\n",
    "print(dir(clusterer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert embeddings to numpy array\nembedding_vectors = np.array([e.embedding for e in test_embeddings])\nprint(f\"Embedding vectors shape: {embedding_vectors.shape}\")\nprint(f\"Expected: (500, 384) - 500 real Java errors, 384-dimensional embeddings\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Explore - fit() method\n",
    "What happens when we fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a fresh clusterer for real data\nclusterer = HDBSCANClusterer(min_cluster_size=5)\n\n# TODO: Call clusterer.fit(embedding_vectors)\n# - What does it return?\n# - What's the shape of labels?\n# - Hints:\n#   - labels should be 1D array with 500 elements\n#   - Each element is a cluster ID (0, 1, 2, ...) or -1 for noise\n#   - Try: print(clusterer.labels)\n#   - Try: print(f\"Unique labels: {sorted(set(clusterer.labels))}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Explore - get_stats() method\n",
    "What statistics are available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClusterStats(num_clusters=0, num_noise_points=20, total_points=20, cluster_sizes={}, avg_cluster_size=0.0, largest_cluster_size=0, smallest_cluster_size=0, noise_fraction=1.0, silhouette_score=None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterer.get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Call clusterer.get_stats() on real data\n# - What does it return?\n# - Print all attributes:\n#   - stats.num_clusters: How many error groups?\n#   - stats.num_noise_points: How many rare/unique errors?\n#   - stats.noise_fraction: Percentage of outliers\n#   - stats.cluster_sizes: Dict of cluster sizes\n#   - stats.avg_cluster_size: Average errors per cluster\n#   - stats.largest_cluster_size: Biggest cluster\n#   - stats.smallest_cluster_size: Smallest cluster\n# \n# You should see real error groupings like NullPointerException, OutOfMemoryError, etc."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Explore - cluster_embeddings() method\n",
    "End-to-end clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# cluster_embeddings() does fit() + creates ClusterAssignment objects\n# It's a convenient wrapper that works with Pydantic models\n\n# TODO: Create new clusterer and call cluster_embeddings(test_embeddings, test_errors)\n# - What does it return?\n# - How is it different from fit()?\n# - What's in each ClusterAssignment object?\n# \n# Try:\n#   clusterer2 = HDBSCANClusterer(min_cluster_size=5)\n#   assignments = clusterer2.cluster_embeddings(test_embeddings, test_errors)\n#   print(f\"Assignments: {len(assignments)}\")\n#   print(f\"First assignment: {assignments[0]}\")\n#   print(f\"  error_id: {assignments[0].error_id}\")\n#   print(f\"  cluster_id: {assignments[0].cluster_id}\")\n#   print(f\"  is_noise: {assignments[0].is_noise}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Explore - Parameters\n",
    "How does min_cluster_size affect results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Try different min_cluster_size values on REAL data\n# - This is the KEY parameter that affects clustering\n# \n# Try:\n#   for min_size in [3, 5, 10, 15]:\n#       c = HDBSCANClusterer(min_cluster_size=min_size)\n#       labels = c.fit(embedding_vectors)\n#       stats = c.get_stats()\n#       \n#       print(f\"\\nmin_cluster_size={min_size}:\")\n#       print(f\"  Clusters: {stats.num_clusters}\")\n#       print(f\"  Noise: {stats.num_noise_points} ({stats.noise_fraction:.1%})\")\n#       print(f\"  Avg size: {stats.avg_cluster_size:.1f}\")\n# \n# Questions:\n# - Smaller min_cluster_size = more or fewer clusters?\n# - How does noise fraction change?\n# - Which value seems best for error clustering?"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
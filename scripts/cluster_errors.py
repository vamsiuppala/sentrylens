#!/usr/bin/env python3
"""
Run HDBSCAN clustering on error embeddings and save cluster assignments.

Usage:
    python scripts/cluster_errors.py --input data/embeddings/embeddings_*.json
    python scripts/cluster_errors.py --input data/embeddings/embeddings_*.json --min-cluster-size 10
    python scripts/cluster_errors.py --input data/embeddings/embeddings_*.json --min-cluster-size 5 --output data/clusters/clusters_*.json
"""
import argparse
import json
from pathlib import Path
from datetime import datetime

from sentrylens.clustering.clusterer import HDBSCANClusterer
from sentrylens.core.models import ErrorEmbedding, AERIErrorRecord
from sentrylens.core.exceptions import ClusteringError
from sentrylens.utils.logger import logger
from sentrylens.config import settings


def load_embeddings_file(file_path: Path) -> list[ErrorEmbedding]:
    """Load embeddings from JSON file generated by generate_embeddings.py."""
    with open(file_path, 'r') as f:
        data = json.load(f)

    embeddings_data = data.get('embeddings', [])
    embeddings = [ErrorEmbedding(**e) for e in embeddings_data]
    return embeddings


def load_errors_jsonl(file_path: Path) -> list[AERIErrorRecord]:
    """Load errors from JSONL file generated by ingest_data.py."""
    import hashlib
    errors = []
    with open(file_path, 'r') as f:
        for line in f:
            raw = json.loads(line)
            # Format stacktrace
            stacktraces = raw.get('stacktraces', [])
            stack_trace_str = "Stack trace unavailable"
            if stacktraces and isinstance(stacktraces[0], list):
                frames = stacktraces[0][:20]
                frame_strs = [
                    f"  at {frame.get('cN', 'Unknown')}.{frame.get('mN', 'method')} ({frame.get('fN', 'file')}:{frame.get('lN', '?')})"
                    for frame in frames if isinstance(frame, dict)
                ]
                stack_trace_str = "Stack trace:\n" + "\n".join(frame_strs)

            error_id = raw.get('error_id')
            if not error_id:
                content = f"{raw.get('summary', '')}:{raw.get('kind', '')}"
                error_id = hashlib.md5(content.encode()).hexdigest()

            errors.append(AERIErrorRecord(
                error_id=error_id,
                error_type=raw.get('kind', 'Unknown'),
                error_message=raw.get('summary', ''),
                stack_trace=stack_trace_str,
                java_version=raw.get('javaRuntimeVersion', ''),
                os_name=raw.get('osgiOs', ''),
            ))
    return errors


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Run HDBSCAN clustering on error embeddings"
    )
    parser.add_argument(
        "--input",
        type=Path,
        required=True,
        help="Path to embeddings JSON file (e.g., embeddings_20260127_141445.json)"
    )
    parser.add_argument(
        "--errors",
        type=Path,
        required=True,
        help="Path to errors JSONL file (e.g., aeri_20260127_141445.jsonl)"
    )
    parser.add_argument(
        "--output",
        type=Path,
        help="Output path for clusters (default: auto-generated)"
    )
    parser.add_argument(
        "--min-cluster-size",
        type=int,
        default=5,
        help="Minimum cluster size (default: 5). Smaller = more clusters."
    )
    parser.add_argument(
        "--min-samples",
        type=int,
        help="Minimum samples in neighborhood (default: same as min-cluster-size)"
    )
    parser.add_argument(
        "--cluster-selection-epsilon",
        type=float,
        default=0.0,
        help="Distance threshold for cluster selection (default: 0.0)"
    )
    parser.add_argument(
        "--metric",
        type=str,
        default="euclidean",
        choices=["euclidean", "cosine", "manhattan"],
        help="Distance metric (default: euclidean)"
    )
    
    return parser.parse_args()


def main():
    """Main clustering pipeline."""
    args = parse_args()
    
    logger.info(
        "Starting clustering",
        input_file=str(args.input),
        min_cluster_size=args.min_cluster_size,
        metric=args.metric
    )
    
    try:
        # Load embeddings from file
        logger.info("Loading embeddings...", file=str(args.input))
        embeddings = load_embeddings_file(args.input)

        if not embeddings:
            logger.error("No embeddings in file. Run generate_embeddings.py first.")
            return 1

        logger.info(f"Loaded {len(embeddings)} embeddings")

        # Load errors from JSONL
        logger.info("Loading errors...", file=str(args.errors))
        errors = load_errors_jsonl(args.errors)
        logger.info(f"Loaded {len(errors)} errors")

        # Initialize clusterer
        logger.info("Initializing HDBSCAN clusterer...")
        clusterer = HDBSCANClusterer(
            min_cluster_size=args.min_cluster_size,
            min_samples=args.min_samples,
            cluster_selection_epsilon=args.cluster_selection_epsilon,
            metric=args.metric
        )

        # Perform clustering
        logger.info("Running clustering...")
        cluster_assignments = clusterer.cluster_embeddings(embeddings)
        
        # Get statistics
        stats = clusterer.get_stats()
        logger.info(
            "Clustering complete",
            num_clusters=stats.num_clusters,
            num_noise_points=stats.num_noise_points,
            total_points=stats.total_points,
            noise_fraction=f"{stats.noise_fraction:.2%}",
            avg_cluster_size=f"{stats.avg_cluster_size:.1f}",
            largest_cluster=stats.largest_cluster_size,
            smallest_cluster=stats.smallest_cluster_size
        )
        
        # Save clusters
        if args.output:
            output_path = args.output
        else:
            # Extract timestamp from input filename (e.g., embeddings_20260127_141445.json)
            timestamp = args.input.stem.replace('embeddings_', '')
            output_path = settings.PROCESSED_DATA_DIR / f"clusters_{timestamp}.json"
        
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        logger.info("Saving cluster assignments...", output_path=str(output_path))
        output_data = {
            'total_clusters': stats.num_clusters,
            'total_points': stats.total_points,
            'noise_points': stats.num_noise_points,
            'clusters': [c.model_dump(mode='json') for c in cluster_assignments],
            'errors': [e.model_dump(mode='json') for e in errors],
            'cluster_sizes': stats.cluster_sizes,
            'source_embeddings_file': str(args.input),
            'source_errors_file': str(args.errors),
            'created_at': datetime.now().isoformat(),
        }
        with open(output_path, 'w') as f:
            json.dump(output_data, f, indent=2, default=str)
        
        logger.info("Cluster assignments saved successfully")
        
        # Print detailed cluster information
        logger.info("\n" + "="*60)
        logger.info("CLUSTER SUMMARY")
        logger.info("="*60)
        
        logger.info(f"Total clusters: {stats.num_clusters}")
        logger.info(f"Noise points: {stats.num_noise_points}")
        logger.info(f"Noise fraction: {stats.noise_fraction:.2%}")
        logger.info(f"Average cluster size: {stats.avg_cluster_size:.1f}")
        logger.info(f"Largest cluster: {stats.largest_cluster_size} errors")
        logger.info(f"Smallest cluster: {stats.smallest_cluster_size} errors")
        
        if stats.num_clusters > 0:
            logger.info("\nCluster size distribution:")
            for cluster_id in sorted(stats.cluster_sizes.keys()):
                size = stats.cluster_sizes[cluster_id]
                percentage = (size / stats.total_points) * 100
                logger.info(f"  Cluster {cluster_id:3d}: {size:4d} errors ({percentage:5.1f}%)")
        
        logger.info("="*60)
        logger.info(f"\nOutput file: {output_path}")
        
        return 0
    
    except ClusteringError as e:
        logger.error(f"Clustering failed: {e}")
        return 1
    except Exception as e:
        logger.exception("Unexpected error during clustering")
        return 1


if __name__ == "__main__":
    exit(main())
